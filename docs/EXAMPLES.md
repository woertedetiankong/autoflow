# AutoFlow ä»£ç ç¤ºä¾‹å’Œä½¿ç”¨åœºæ™¯

## ğŸ“‹ ç›®å½•

- [Python SDKç¤ºä¾‹](#python-sdkç¤ºä¾‹)
- [REST APIç¤ºä¾‹](#rest-apiç¤ºä¾‹)
- [ä½¿ç”¨åœºæ™¯](#ä½¿ç”¨åœºæ™¯)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)

## ğŸ Python SDKç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨

#### 1. åˆå§‹åŒ–å’Œé…ç½®

```python
import os
from autoflow import Autoflow
from autoflow.configs.db import DatabaseConfig
from autoflow.configs.main import Config
from autoflow.models.llms import LLM
from autoflow.models.embedding_models import EmbeddingModel
from autoflow.configs.knowledge_base import IndexMethod

# ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®
def create_autoflow():
    return Autoflow.from_config(
        config=Config(
            db=DatabaseConfig(
                host=os.getenv("TIDB_HOST"),
                port=int(os.getenv("TIDB_PORT", "4000")),
                username=os.getenv("TIDB_USERNAME"),
                password=os.getenv("TIDB_PASSWORD"),
                database=os.getenv("TIDB_DATABASE"),
                enable_ssl=os.getenv("TIDB_SSL_ENABLED", "false").lower() == "true",
            )
        )
    )

# åˆ›å»ºAutoFlowå®ä¾‹
af = create_autoflow()
```

#### 2. åˆ›å»ºçŸ¥è¯†åº“

```python
from autoflow.chunkers.text import TextChunker
from autoflow.configs.chunkers.text import TextChunkerConfig

# é…ç½®æ–‡æœ¬åˆ†å—å™¨
chunker = TextChunker(
    config=TextChunkerConfig(
        chunk_size=512,      # åˆ†å—å¤§å°
        chunk_overlap=50,    # é‡å å¤§å°
        separator="\n\n"     # åˆ†éš”ç¬¦
    )
)

# åˆ›å»ºçŸ¥è¯†åº“
kb = af.create_knowledge_base(
    name="ä¼ä¸šçŸ¥è¯†åº“",
    description="åŒ…å«å…¬å¸æ”¿ç­–ã€äº§å“æ–‡æ¡£å’ŒFAQ",
    index_methods=[
        IndexMethod.VECTOR_SEARCH,    # å‘é‡æœç´¢
        IndexMethod.KNOWLEDGE_GRAPH   # çŸ¥è¯†å›¾è°±
    ],
    llm=LLM("gpt-4o-mini"),
    embedding_model=EmbeddingModel("text-embedding-3-small"),
)

print(f"çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸ: {kb.name}")
```

#### 3. æ‰¹é‡æ·»åŠ æ–‡æ¡£

```python
import glob
from pathlib import Path

def add_documents_from_directory(kb, directory_path, file_patterns=None):
    """ä»ç›®å½•æ‰¹é‡æ·»åŠ æ–‡æ¡£"""
    if file_patterns is None:
        file_patterns = ["*.pdf", "*.md", "*.txt", "*.docx"]
    
    all_files = []
    for pattern in file_patterns:
        all_files.extend(glob.glob(f"{directory_path}/**/{pattern}", recursive=True))
    
    print(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")
    
    # æ‰¹é‡å¤„ç†æ–‡æ¡£
    batch_size = 10
    for i in range(0, len(all_files), batch_size):
        batch_files = all_files[i:i + batch_size]
        print(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch_files)} ä¸ªæ–‡ä»¶")
        
        try:
            documents = kb.add(batch_files, chunker=chunker)
            print(f"æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£")
        except Exception as e:
            print(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
            # é€ä¸ªå¤„ç†å¤±è´¥çš„æ–‡ä»¶
            for file_path in batch_files:
                try:
                    kb.add(file_path, chunker=chunker)
                    print(f"å•ç‹¬å¤„ç†æˆåŠŸ: {file_path}")
                except Exception as file_error:
                    print(f"æ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {file_error}")

# ä½¿ç”¨ç¤ºä¾‹
add_documents_from_directory(kb, "./documents")
```

#### 4. æ™ºèƒ½æœç´¢å’Œé—®ç­”

```python
def intelligent_search(kb, query, use_kg=True):
    """æ™ºèƒ½æœç´¢å‡½æ•°"""
    print(f"æœç´¢æŸ¥è¯¢: {query}")
    
    # 1. å‘é‡æœç´¢
    vector_results = kb.search_documents(
        query=query,
        top_k=5,
        similarity_threshold=0.7
    )
    
    print(f"å‘é‡æœç´¢æ‰¾åˆ° {len(vector_results.chunks)} ä¸ªç›¸å…³ç‰‡æ®µ")
    
    # 2. çŸ¥è¯†å›¾è°±æœç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if use_kg:
        kg_results = kb.search_knowledge_graph(
            query=query,
            depth=2
        )
        print(f"çŸ¥è¯†å›¾è°±æ‰¾åˆ° {len(kg_results.relationships)} ä¸ªå…³ç³»")
    
    # 3. ç”Ÿæˆç­”æ¡ˆ
    response = kb.ask(query)
    
    return {
        "answer": response.message.content,
        "vector_results": vector_results,
        "kg_results": kg_results if use_kg else None
    }

# ä½¿ç”¨ç¤ºä¾‹
result = intelligent_search(kb, "å…¬å¸çš„ä¼‘å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ")
print(f"ç­”æ¡ˆ: {result['answer']}")
```

### é«˜çº§åŠŸèƒ½ç¤ºä¾‹

#### 1. è‡ªå®šä¹‰æ–‡æ¡£å¤„ç†å™¨

```python
from autoflow.loaders.base import Loader
from autoflow.data_types import DataType, Document
import json

class JSONLoader(Loader):
    """è‡ªå®šä¹‰JSONæ–‡æ¡£åŠ è½½å™¨"""
    
    def load(self, source):
        documents = []
        
        if isinstance(source, str):
            source = [source]
        
        for file_path in source:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # å°†JSONè½¬æ¢ä¸ºæ–‡æ¡£
            if isinstance(data, list):
                for i, item in enumerate(data):
                    doc = Document(
                        text=json.dumps(item, ensure_ascii=False, indent=2),
                        metadata={
                            "source": file_path,
                            "index": i,
                            "type": "json_item"
                        }
                    )
                    documents.append(doc)
            else:
                doc = Document(
                    text=json.dumps(data, ensure_ascii=False, indent=2),
                    metadata={
                        "source": file_path,
                        "type": "json_document"
                    }
                )
                documents.append(doc)
        
        return documents

# ä½¿ç”¨è‡ªå®šä¹‰åŠ è½½å™¨
json_loader = JSONLoader()
documents = kb.add("./data.json", loader=json_loader)
```

#### 2. å¤šæ¨¡æ€æ–‡æ¡£å¤„ç†

```python
from autoflow.loaders.file import FileLoader
from autoflow.data_types import DataType

def process_mixed_documents(kb, file_paths):
    """å¤„ç†æ··åˆç±»å‹çš„æ–‡æ¡£"""
    results = {}
    
    for file_path in file_paths:
        try:
            # è‡ªåŠ¨æ£€æµ‹æ–‡æ¡£ç±»å‹
            data_type = guess_datatype(file_path)
            print(f"å¤„ç†æ–‡ä»¶: {file_path}, ç±»å‹: {data_type}")
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ä¸åŒçš„å¤„ç†ç­–ç•¥
            if data_type == DataType.PDF:
                # PDFæ–‡æ¡£ä½¿ç”¨è¾ƒå¤§çš„åˆ†å—
                pdf_chunker = TextChunker(
                    config=TextChunkerConfig(chunk_size=1024, chunk_overlap=100)
                )
                docs = kb.add(file_path, chunker=pdf_chunker)
            elif data_type == DataType.MARKDOWN:
                # Markdownä¿æŒç»“æ„
                md_chunker = TextChunker(
                    config=TextChunkerConfig(
                        chunk_size=512, 
                        chunk_overlap=50,
                        separator="\n## "  # æŒ‰æ ‡é¢˜åˆ†å‰²
                    )
                )
                docs = kb.add(file_path, chunker=md_chunker)
            else:
                # å…¶ä»–ç±»å‹ä½¿ç”¨é»˜è®¤è®¾ç½®
                docs = kb.add(file_path)
            
            results[file_path] = {
                "status": "success",
                "document_count": len(docs),
                "chunk_count": sum(len(doc.chunks) for doc in docs)
            }
            
        except Exception as e:
            results[file_path] = {
                "status": "error",
                "error": str(e)
            }
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
mixed_files = [
    "./manual.pdf",
    "./readme.md", 
    "./config.json",
    "./data.csv"
]
results = process_mixed_documents(kb, mixed_files)
```

#### 3. å®æ—¶æ–‡æ¡£ç›‘æ§å’Œæ›´æ–°

```python
import time
import hashlib
from pathlib import Path
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class DocumentWatcher(FileSystemEventHandler):
    """æ–‡æ¡£å˜åŒ–ç›‘æ§å™¨"""
    
    def __init__(self, knowledge_base, watch_directory):
        self.kb = knowledge_base
        self.watch_dir = Path(watch_directory)
        self.file_hashes = {}
        
        # åˆå§‹åŒ–æ–‡ä»¶å“ˆå¸Œ
        self._scan_directory()
    
    def _scan_directory(self):
        """æ‰«æç›®å½•å¹¶è®¡ç®—æ–‡ä»¶å“ˆå¸Œ"""
        for file_path in self.watch_dir.rglob("*"):
            if file_path.is_file() and file_path.suffix in ['.pdf', '.md', '.txt']:
                self.file_hashes[str(file_path)] = self._get_file_hash(file_path)
    
    def _get_file_hash(self, file_path):
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def on_modified(self, event):
        if not event.is_directory:
            self._handle_file_change(event.src_path, "modified")
    
    def on_created(self, event):
        if not event.is_directory:
            self._handle_file_change(event.src_path, "created")
    
    def on_deleted(self, event):
        if not event.is_directory:
            self._handle_file_change(event.src_path, "deleted")
    
    def _handle_file_change(self, file_path, change_type):
        """å¤„ç†æ–‡ä»¶å˜åŒ–"""
        try:
            if change_type == "deleted":
                # ä»çŸ¥è¯†åº“ä¸­åˆ é™¤æ–‡æ¡£
                print(f"æ–‡ä»¶åˆ é™¤: {file_path}")
                # è¿™é‡Œéœ€è¦å®ç°åˆ é™¤é€»è¾‘
                if file_path in self.file_hashes:
                    del self.file_hashes[file_path]
            
            elif change_type in ["created", "modified"]:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦çœŸçš„å‘ç”Ÿäº†å˜åŒ–
                current_hash = self._get_file_hash(file_path)
                old_hash = self.file_hashes.get(file_path)
                
                if current_hash != old_hash:
                    print(f"æ–‡ä»¶{change_type}: {file_path}")
                    
                    # é‡æ–°æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
                    self.kb.add(file_path)
                    self.file_hashes[file_path] = current_hash
                    
                    print(f"æ–‡æ¡£å·²æ›´æ–°åˆ°çŸ¥è¯†åº“: {file_path}")
        
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶å˜åŒ–å¤±è´¥ {file_path}: {e}")

def start_document_monitoring(kb, watch_directory):
    """å¯åŠ¨æ–‡æ¡£ç›‘æ§"""
    event_handler = DocumentWatcher(kb, watch_directory)
    observer = Observer()
    observer.schedule(event_handler, watch_directory, recursive=True)
    observer.start()
    
    print(f"å¼€å§‹ç›‘æ§ç›®å½•: {watch_directory}")
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
        print("åœæ­¢ç›‘æ§")
    
    observer.join()

# ä½¿ç”¨ç¤ºä¾‹ï¼ˆåœ¨å•ç‹¬çš„è¿›ç¨‹ä¸­è¿è¡Œï¼‰
# start_document_monitoring(kb, "./documents")
```

## ğŸŒ REST APIç¤ºä¾‹

### ä½¿ç”¨Python requests

#### 1. åŸºç¡€èŠå¤©APIè°ƒç”¨

```python
import requests
import json

class AutoFlowClient:
    def __init__(self, base_url, api_key=None):
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()
        if api_key:
            self.session.headers.update({
                'Authorization': f'Bearer {api_key}'
            })
    
    def chat(self, messages, chat_engine="default", stream=True):
        """å‘èµ·èŠå¤©"""
        url = f"{self.base_url}/api/chats"
        data = {
            "messages": messages,
            "chat_engine": chat_engine,
            "stream": stream
        }
        
        if stream:
            return self._stream_chat(url, data)
        else:
            response = self.session.post(url, json=data)
            response.raise_for_status()
            return response.json()
    
    def _stream_chat(self, url, data):
        """å¤„ç†æµå¼èŠå¤©å“åº”"""
        response = self.session.post(url, json=data, stream=True)
        response.raise_for_status()
        
        for line in response.iter_lines():
            if line:
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    try:
                        event_data = json.loads(line[6:])
                        yield event_data
                    except json.JSONDecodeError:
                        continue
    
    def get_chat_history(self, chat_id):
        """è·å–èŠå¤©å†å²"""
        url = f"{self.base_url}/api/chats/{chat_id}"
        response = self.session.get(url)
        response.raise_for_status()
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
client = AutoFlowClient("https://your-domain.com", "your-api-key")

# éæµå¼èŠå¤©
messages = [{"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}]
result = client.chat(messages, stream=False)
print(f"å›ç­”: {result['message']['content']}")

# æµå¼èŠå¤©
print("æµå¼å›ç­”:")
for event in client.chat(messages, stream=True):
    if event.get('event_type') == 'text_part':
        print(event['payload'], end='', flush=True)
    elif event.get('event_type') == 'done':
        print("\n[å®Œæˆ]")
        break
```

#### 2. æ–‡æ¡£æ£€ç´¢API

```python
def search_documents(client, query, knowledge_base_ids=None, top_k=5):
    """æœç´¢æ–‡æ¡£"""
    url = f"{client.base_url}/api/retrieve/documents"
    data = {
        "query": query,
        "top_k": top_k,
        "similarity_threshold": 0.7
    }
    
    if knowledge_base_ids:
        data["knowledge_base_ids"] = knowledge_base_ids
    
    response = client.session.post(url, json=data)
    response.raise_for_status()
    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
search_results = search_documents(
    client, 
    "äººå·¥æ™ºèƒ½çš„åº”ç”¨é¢†åŸŸ", 
    knowledge_base_ids=[1, 2],
    top_k=3
)

print(f"æ‰¾åˆ° {len(search_results['chunks'])} ä¸ªç›¸å…³ç‰‡æ®µ:")
for chunk in search_results['chunks']:
    print(f"- ç›¸å…³åº¦: {chunk['score']:.3f}")
    print(f"  å†…å®¹: {chunk['text'][:100]}...")
    print(f"  æ¥æº: {chunk['document']['name']}")
```

### ä½¿ç”¨JavaScript/Node.js

#### 1. åŸºç¡€èŠå¤©å®¢æˆ·ç«¯

```javascript
class AutoFlowClient {
    constructor(baseUrl, apiKey = null) {
        this.baseUrl = baseUrl.replace(/\/$/, '');
        this.apiKey = apiKey;
    }

    async chat(messages, options = {}) {
        const {
            chatEngine = 'default',
            stream = true,
            chatId = null
        } = options;

        const url = `${this.baseUrl}/api/chats`;
        const data = {
            messages,
            chat_engine: chatEngine,
            stream,
            ...(chatId && { chat_id: chatId })
        };

        const headers = {
            'Content-Type': 'application/json',
            ...(this.apiKey && { 'Authorization': `Bearer ${this.apiKey}` })
        };

        if (stream) {
            return this._streamChat(url, data, headers);
        } else {
            const response = await fetch(url, {
                method: 'POST',
                headers,
                body: JSON.stringify(data)
            });
            
            if (!response.ok) {
                throw new Error(`HTTP ${response.status}: ${response.statusText}`);
            }
            
            return await response.json();
        }
    }

    async *_streamChat(url, data, headers) {
        const response = await fetch(url, {
            method: 'POST',
            headers,
            body: JSON.stringify(data)
        });

        if (!response.ok) {
            throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }

        const reader = response.body.getReader();
        const decoder = new TextDecoder();

        try {
            while (true) {
                const { done, value } = await reader.read();
                if (done) break;

                const chunk = decoder.decode(value);
                const lines = chunk.split('\n');

                for (const line of lines) {
                    if (line.startsWith('data: ')) {
                        try {
                            const eventData = JSON.parse(line.slice(6));
                            yield eventData;
                        } catch (e) {
                            // å¿½ç•¥è§£æé”™è¯¯
                        }
                    }
                }
            }
        } finally {
            reader.releaseLock();
        }
    }

    async getChatHistory(chatId) {
        const url = `${this.baseUrl}/api/chats/${chatId}`;
        const headers = {
            ...(this.apiKey && { 'Authorization': `Bearer ${this.apiKey}` })
        };

        const response = await fetch(url, { headers });
        
        if (!response.ok) {
            throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }
        
        return await response.json();
    }
}

// ä½¿ç”¨ç¤ºä¾‹
const client = new AutoFlowClient('https://your-domain.com', 'your-api-key');

// éæµå¼èŠå¤©
async function simpleChat() {
    const messages = [{ role: 'user', content: 'ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ' }];
    const result = await client.chat(messages, { stream: false });
    console.log('å›ç­”:', result.message.content);
}

// æµå¼èŠå¤©
async function streamChat() {
    const messages = [{ role: 'user', content: 'è§£é‡Šä¸€ä¸‹ç¥ç»ç½‘ç»œ' }];
    console.log('æµå¼å›ç­”:');
    
    for await (const event of client.chat(messages, { stream: true })) {
        if (event.event_type === 'text_part') {
            process.stdout.write(event.payload);
        } else if (event.event_type === 'done') {
            console.log('\n[å®Œæˆ]');
            break;
        }
    }
}

// è¿è¡Œç¤ºä¾‹
simpleChat().catch(console.error);
streamChat().catch(console.error);
```

#### 2. ç½‘é¡µé›†æˆç¤ºä¾‹

```html
<!DOCTYPE html>
<html>
<head>
    <title>AutoFlow èŠå¤©ç¤ºä¾‹</title>
    <style>
        .chat-container {
            max-width: 600px;
            margin: 0 auto;
            padding: 20px;
        }
        .message {
            margin: 10px 0;
            padding: 10px;
            border-radius: 5px;
        }
        .user-message {
            background-color: #e3f2fd;
            text-align: right;
        }
        .assistant-message {
            background-color: #f5f5f5;
        }
        .input-container {
            display: flex;
            margin-top: 20px;
        }
        #messageInput {
            flex: 1;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        #sendButton {
            padding: 10px 20px;
            margin-left: 10px;
            background-color: #2196f3;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
    </style>
</head>
<body>
    <div class="chat-container">
        <h1>AutoFlow èŠå¤©æ¼”ç¤º</h1>
        <div id="chatMessages"></div>
        <div class="input-container">
            <input type="text" id="messageInput" placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜...">
            <button id="sendButton">å‘é€</button>
        </div>
    </div>

    <script>
        class ChatUI {
            constructor(apiBaseUrl, apiKey) {
                this.client = new AutoFlowClient(apiBaseUrl, apiKey);
                this.messagesContainer = document.getElementById('chatMessages');
                this.messageInput = document.getElementById('messageInput');
                this.sendButton = document.getElementById('sendButton');
                
                this.setupEventListeners();
            }

            setupEventListeners() {
                this.sendButton.addEventListener('click', () => this.sendMessage());
                this.messageInput.addEventListener('keypress', (e) => {
                    if (e.key === 'Enter') {
                        this.sendMessage();
                    }
                });
            }

            addMessage(content, isUser = false) {
                const messageDiv = document.createElement('div');
                messageDiv.className = `message ${isUser ? 'user-message' : 'assistant-message'}`;
                messageDiv.textContent = content;
                this.messagesContainer.appendChild(messageDiv);
                this.messagesContainer.scrollTop = this.messagesContainer.scrollHeight;
                return messageDiv;
            }

            async sendMessage() {
                const message = this.messageInput.value.trim();
                if (!message) return;

                // æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
                this.addMessage(message, true);
                this.messageInput.value = '';
                this.sendButton.disabled = true;

                // åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯å®¹å™¨
                const assistantMessageDiv = this.addMessage('', false);
                let assistantMessage = '';

                try {
                    const messages = [{ role: 'user', content: message }];
                    
                    for await (const event of this.client.chat(messages, { stream: true })) {
                        if (event.event_type === 'text_part') {
                            assistantMessage += event.payload;
                            assistantMessageDiv.textContent = assistantMessage;
                        } else if (event.event_type === 'done') {
                            break;
                        }
                    }
                } catch (error) {
                    assistantMessageDiv.textContent = `é”™è¯¯: ${error.message}`;
                    assistantMessageDiv.style.color = 'red';
                } finally {
                    this.sendButton.disabled = false;
                }
            }
        }

        // åˆå§‹åŒ–èŠå¤©ç•Œé¢
        const chatUI = new ChatUI('https://your-domain.com', 'your-api-key');
    </script>
</body>
</html>

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### 1. ä¼ä¸šçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ

æ„å»ºä¼ä¸šå†…éƒ¨çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œå¸®åŠ©å‘˜å·¥å¿«é€Ÿæ‰¾åˆ°æ‰€éœ€ä¿¡æ¯ã€‚

```python
class EnterpriseKnowledgeBase:
    def __init__(self, autoflow_instance):
        self.af = autoflow_instance
        self.departments = {}

    def create_department_kb(self, dept_name, documents_path):
        """ä¸ºç‰¹å®šéƒ¨é—¨åˆ›å»ºçŸ¥è¯†åº“"""
        kb = self.af.create_knowledge_base(
            name=f"{dept_name}çŸ¥è¯†åº“",
            description=f"{dept_name}éƒ¨é—¨çš„æ”¿ç­–ã€æµç¨‹å’ŒFAQ",
            llm=LLM("gpt-4o-mini"),
            embedding_model=EmbeddingModel("text-embedding-3-small"),
        )

        # æ·»åŠ éƒ¨é—¨æ–‡æ¡£
        kb.add(documents_path)
        self.departments[dept_name] = kb

        return kb

    def smart_routing(self, question):
        """æ™ºèƒ½è·¯ç”±é—®é¢˜åˆ°ç›¸å…³éƒ¨é—¨"""
        # ç®€å•çš„å…³é”®è¯è·¯ç”±ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„åˆ†ç±»æ¨¡å‹ï¼‰
        routing_keywords = {
            "äººäº‹": ["å·¥èµ„", "è–ªèµ„", "è¯·å‡", "æ‹›è˜", "ç¦»èŒ", "è€ƒå‹¤"],
            "IT": ["ç³»ç»Ÿ", "è½¯ä»¶", "ç½‘ç»œ", "å¯†ç ", "æƒé™", "ç”µè„‘"],
            "è´¢åŠ¡": ["æŠ¥é”€", "å‘ç¥¨", "é¢„ç®—", "æˆæœ¬", "ä¼šè®¡"],
            "æ³•åŠ¡": ["åˆåŒ", "æ³•å¾‹", "åˆè§„", "é£é™©", "çŸ¥è¯†äº§æƒ"]
        }

        question_lower = question.lower()
        for dept, keywords in routing_keywords.items():
            if any(keyword in question_lower for keyword in keywords):
                if dept in self.departments:
                    return self.departments[dept]

        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç‰¹å®šéƒ¨é—¨ï¼Œä½¿ç”¨é€šç”¨çŸ¥è¯†åº“
        return self.departments.get("é€šç”¨", None)

    def answer_question(self, question, user_department=None):
        """å›ç­”é—®é¢˜"""
        # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æ‰€åœ¨éƒ¨é—¨çš„çŸ¥è¯†åº“
        if user_department and user_department in self.departments:
            primary_kb = self.departments[user_department]
        else:
            primary_kb = self.smart_routing(question)

        if primary_kb:
            response = primary_kb.ask(question)
            return {
                "answer": response.message.content,
                "source_department": primary_kb.name,
                "confidence": "high"
            }
        else:
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·è”ç³»ç›¸å…³éƒ¨é—¨æˆ–ç®¡ç†å‘˜ã€‚",
                "source_department": None,
                "confidence": "low"
            }

# ä½¿ç”¨ç¤ºä¾‹
enterprise_kb = EnterpriseKnowledgeBase(af)

# åˆ›å»ºå„éƒ¨é—¨çŸ¥è¯†åº“
enterprise_kb.create_department_kb("äººäº‹", "./hr_documents/")
enterprise_kb.create_department_kb("IT", "./it_documents/")
enterprise_kb.create_department_kb("è´¢åŠ¡", "./finance_documents/")

# å›ç­”é—®é¢˜
result = enterprise_kb.answer_question("å¦‚ä½•ç”³è¯·å¹´å‡ï¼Ÿ", user_department="äººäº‹")
print(f"å›ç­”: {result['answer']}")
```

### 2. å®¢æˆ·æœåŠ¡èŠå¤©æœºå™¨äºº

ä¸ºå®¢æˆ·æœåŠ¡æ„å»ºæ™ºèƒ½èŠå¤©æœºå™¨äººï¼Œæä¾›24/7çš„å®¢æˆ·æ”¯æŒã€‚

```python
class CustomerServiceBot:
    def __init__(self, knowledge_base):
        self.kb = knowledge_base
        self.conversation_history = {}
        self.escalation_keywords = [
            "æŠ•è¯‰", "ä¸æ»¡æ„", "é€€æ¬¾", "å–æ¶ˆè®¢å•", "äººå·¥å®¢æœ"
        ]

    def should_escalate(self, message):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è½¬äººå·¥å®¢æœ"""
        return any(keyword in message for keyword in self.escalation_keywords)

    def get_context_aware_response(self, user_id, message):
        """è·å–ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›å¤"""
        # è·å–å¯¹è¯å†å²
        history = self.conversation_history.get(user_id, [])

        # æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
        context_messages = history + [{"role": "user", "content": message}]

        # å¦‚æœéœ€è¦è½¬äººå·¥å®¢æœ
        if self.should_escalate(message):
            return {
                "response": "æˆ‘ç†è§£æ‚¨çš„é—®é¢˜å¾ˆé‡è¦ã€‚è®©æˆ‘ä¸ºæ‚¨è½¬æ¥äººå·¥å®¢æœï¼Œä»–ä»¬ä¼šæ›´å¥½åœ°å¸®åŠ©æ‚¨è§£å†³é—®é¢˜ã€‚",
                "action": "escalate_to_human",
                "confidence": 1.0
            }

        # ä½¿ç”¨çŸ¥è¯†åº“å›ç­”
        kb_response = self.kb.ask(message)

        # æ›´æ–°å¯¹è¯å†å²
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": kb_response.message.content})

        # ä¿æŒå†å²è®°å½•ä¸è¶…è¿‡10è½®å¯¹è¯
        if len(history) > 20:
            history = history[-20:]

        self.conversation_history[user_id] = history

        return {
            "response": kb_response.message.content,
            "action": "continue_conversation",
            "confidence": 0.8  # å¯ä»¥æ ¹æ®æ£€ç´¢ç»“æœçš„ç›¸å…³åº¦è®¡ç®—
        }

    def handle_customer_query(self, user_id, message):
        """å¤„ç†å®¢æˆ·æŸ¥è¯¢"""
        try:
            result = self.get_context_aware_response(user_id, message)

            # è®°å½•å¯¹è¯æ—¥å¿—
            self.log_conversation(user_id, message, result)

            return result

        except Exception as e:
            return {
                "response": "æŠ±æ­‰ï¼Œç³»ç»Ÿæš‚æ—¶å‡ºç°é—®é¢˜ã€‚è¯·ç¨åå†è¯•æˆ–è”ç³»äººå·¥å®¢æœã€‚",
                "action": "system_error",
                "error": str(e)
            }

    def log_conversation(self, user_id, query, response):
        """è®°å½•å¯¹è¯æ—¥å¿—ï¼ˆç”¨äºåˆ†æå’Œæ”¹è¿›ï¼‰"""
        log_entry = {
            "timestamp": time.time(),
            "user_id": user_id,
            "query": query,
            "response": response["response"],
            "action": response["action"],
            "confidence": response.get("confidence", 0)
        }
        # è¿™é‡Œå¯ä»¥å†™å…¥æ•°æ®åº“æˆ–æ—¥å¿—æ–‡ä»¶
        print(f"å¯¹è¯æ—¥å¿—: {log_entry}")

# ä½¿ç”¨ç¤ºä¾‹
customer_bot = CustomerServiceBot(kb)

# æ¨¡æ‹Ÿå®¢æˆ·å¯¹è¯
user_id = "customer_123"
queries = [
    "ä½ ä»¬çš„é€€è´§æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ",
    "æˆ‘æƒ³é€€è´§ï¼Œä½†æ˜¯å·²ç»è¶…è¿‡7å¤©äº†",
    "è¿™ä¸åˆç†ï¼Œæˆ‘è¦æŠ•è¯‰"
]

for query in queries:
    result = customer_bot.handle_customer_query(user_id, query)
    print(f"å®¢æˆ·: {query}")
    print(f"æœºå™¨äºº: {result['response']}")
    print(f"åŠ¨ä½œ: {result['action']}")
    print("---")
```

### 3. æ•™è‚²åŸ¹è®­åŠ©æ‰‹

ä¸ºåœ¨çº¿æ•™è‚²å¹³å°æ„å»ºæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ã€‚

```python
class EducationAssistant:
    def __init__(self, autoflow_instance):
        self.af = autoflow_instance
        self.course_kbs = {}
        self.student_progress = {}

    def create_course_kb(self, course_id, course_materials):
        """ä¸ºè¯¾ç¨‹åˆ›å»ºçŸ¥è¯†åº“"""
        kb = self.af.create_knowledge_base(
            name=f"è¯¾ç¨‹_{course_id}",
            description=f"è¯¾ç¨‹{course_id}çš„æ•™å­¦ææ–™å’Œå‚è€ƒèµ„æ–™",
            llm=LLM("gpt-4o-mini"),
            embedding_model=EmbeddingModel("text-embedding-3-small"),
        )

        # æ·»åŠ è¯¾ç¨‹ææ–™
        kb.add(course_materials)
        self.course_kbs[course_id] = kb

        return kb

    def adaptive_learning(self, student_id, course_id, question, difficulty_level="medium"):
        """è‡ªé€‚åº”å­¦ä¹ å›ç­”"""
        if course_id not in self.course_kbs:
            return "è¯¾ç¨‹ä¸å­˜åœ¨"

        kb = self.course_kbs[course_id]

        # æ ¹æ®éš¾åº¦çº§åˆ«è°ƒæ•´å›ç­”é£æ ¼
        difficulty_prompts = {
            "beginner": "è¯·ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šï¼ŒåŒ…å«å…·ä½“ä¾‹å­ï¼š",
            "medium": "è¯·è¯¦ç»†è§£é‡Šæ¦‚å¿µå’ŒåŸç†ï¼š",
            "advanced": "è¯·æ·±å…¥åˆ†æï¼ŒåŒ…å«ç›¸å…³ç†è®ºå’Œåº”ç”¨ï¼š"
        }

        enhanced_question = difficulty_prompts.get(difficulty_level, "") + question
        response = kb.ask(enhanced_question)

        # è®°å½•å­¦ä¹ è¿›åº¦
        self.update_student_progress(student_id, course_id, question, difficulty_level)

        return response.message.content

    def update_student_progress(self, student_id, course_id, question, difficulty):
        """æ›´æ–°å­¦ç”Ÿå­¦ä¹ è¿›åº¦"""
        if student_id not in self.student_progress:
            self.student_progress[student_id] = {}

        if course_id not in self.student_progress[student_id]:
            self.student_progress[student_id][course_id] = {
                "questions_asked": 0,
                "topics_covered": set(),
                "difficulty_distribution": {"beginner": 0, "medium": 0, "advanced": 0}
            }

        progress = self.student_progress[student_id][course_id]
        progress["questions_asked"] += 1
        progress["difficulty_distribution"][difficulty] += 1

        # ç®€å•çš„ä¸»é¢˜æå–ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨NLPæŠ€æœ¯ï¼‰
        topics = self.extract_topics(question)
        progress["topics_covered"].update(topics)

    def extract_topics(self, question):
        """æå–é—®é¢˜ä¸­çš„ä¸»é¢˜ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæŠ€æœ¯
        common_topics = {
            "æœºå™¨å­¦ä¹ ": ["æœºå™¨å­¦ä¹ ", "ML", "ç®—æ³•", "æ¨¡å‹"],
            "æ·±åº¦å­¦ä¹ ": ["æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "DL", "CNN", "RNN"],
            "æ•°æ®ç§‘å­¦": ["æ•°æ®åˆ†æ", "ç»Ÿè®¡", "å¯è§†åŒ–", "pandas"],
            "ç¼–ç¨‹": ["Python", "ä»£ç ", "å‡½æ•°", "å˜é‡"]
        }

        found_topics = []
        question_lower = question.lower()

        for topic, keywords in common_topics.items():
            if any(keyword.lower() in question_lower for keyword in keywords):
                found_topics.append(topic)

        return found_topics

    def generate_study_plan(self, student_id, course_id):
        """ç”Ÿæˆä¸ªæ€§åŒ–å­¦ä¹ è®¡åˆ’"""
        if student_id not in self.student_progress:
            return "æš‚æ— å­¦ä¹ è®°å½•"

        progress = self.student_progress[student_id].get(course_id, {})

        if not progress:
            return "æš‚æ— è¯¥è¯¾ç¨‹çš„å­¦ä¹ è®°å½•"

        # åˆ†æå­¦ä¹ æƒ…å†µ
        total_questions = progress["questions_asked"]
        topics_covered = len(progress["topics_covered"])
        difficulty_dist = progress["difficulty_distribution"]

        # ç”Ÿæˆå»ºè®®
        suggestions = []

        if difficulty_dist["beginner"] > difficulty_dist["medium"] + difficulty_dist["advanced"]:
            suggestions.append("å»ºè®®å°è¯•æ›´æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜æ¥æå‡å­¦ä¹ æ·±åº¦")

        if topics_covered < 3:
            suggestions.append("å»ºè®®æ‰©å±•å­¦ä¹ èŒƒå›´ï¼Œæ¢ç´¢æ›´å¤šç›¸å…³ä¸»é¢˜")

        if total_questions < 10:
            suggestions.append("å»ºè®®å¢åŠ ç»ƒä¹ é¢‘ç‡ï¼Œå¤šæé—®å¤šæ€è€ƒ")

        return {
            "å­¦ä¹ ç»Ÿè®¡": {
                "æ€»æé—®æ•°": total_questions,
                "æ¶‰åŠä¸»é¢˜": list(progress["topics_covered"]),
                "éš¾åº¦åˆ†å¸ƒ": difficulty_dist
            },
            "å­¦ä¹ å»ºè®®": suggestions
        }

# ä½¿ç”¨ç¤ºä¾‹
edu_assistant = EducationAssistant(af)

# åˆ›å»ºè¯¾ç¨‹çŸ¥è¯†åº“
edu_assistant.create_course_kb("ML101", "./ml_course_materials/")

# å­¦ç”Ÿæé—®
student_id = "student_456"
course_id = "ML101"

questions = [
    ("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ", "beginner"),
    ("ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„åŒºåˆ«", "medium"),
    ("æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ•°å­¦åŸç†", "advanced")
]

for question, difficulty in questions:
    answer = edu_assistant.adaptive_learning(student_id, course_id, question, difficulty)
    print(f"é—®é¢˜ ({difficulty}): {question}")
    print(f"å›ç­”: {answer[:200]}...")
    print("---")

# ç”Ÿæˆå­¦ä¹ è®¡åˆ’
study_plan = edu_assistant.generate_study_plan(student_id, course_id)
print("å­¦ä¹ è®¡åˆ’:", study_plan)
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ–‡æ¡£é¢„å¤„ç†

```python
def preprocess_documents(file_paths):
    """æ–‡æ¡£é¢„å¤„ç†æœ€ä½³å®è·µ"""
    processed_docs = []

    for file_path in file_paths:
        try:
            # 1. æ–‡ä»¶æ ¼å¼éªŒè¯
            if not is_supported_format(file_path):
                print(f"è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
                continue

            # 2. æ–‡ä»¶å¤§å°æ£€æŸ¥
            file_size = os.path.getsize(file_path)
            if file_size > 50 * 1024 * 1024:  # 50MB
                print(f"æ–‡ä»¶è¿‡å¤§ï¼Œå»ºè®®åˆ†å‰²: {file_path}")
                continue

            # 3. æ–‡æœ¬è´¨é‡æ£€æŸ¥
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            if len(content.strip()) < 100:
                print(f"æ–‡æ¡£å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡: {file_path}")
                continue

            # 4. å»é™¤å™ªå£°å†…å®¹
            cleaned_content = clean_text(content)

            # 5. æ·»åŠ å…ƒæ•°æ®
            metadata = extract_metadata(file_path, cleaned_content)

            processed_docs.append({
                "path": file_path,
                "content": cleaned_content,
                "metadata": metadata
            })

        except Exception as e:
            print(f"å¤„ç†æ–‡æ¡£å¤±è´¥ {file_path}: {e}")

    return processed_docs

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬å†…å®¹"""
    import re

    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)

    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™åŸºæœ¬æ ‡ç‚¹ï¼‰
    text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()[\]{}"\'-]', '', text)

    # ç§»é™¤è¿‡çŸ­çš„è¡Œ
    lines = text.split('\n')
    lines = [line.strip() for line in lines if len(line.strip()) > 10]

    return '\n'.join(lines)

def extract_metadata(file_path, content):
    """æå–æ–‡æ¡£å…ƒæ•°æ®"""
    from datetime import datetime
    import hashlib

    return {
        "file_name": os.path.basename(file_path),
        "file_size": os.path.getsize(file_path),
        "content_hash": hashlib.md5(content.encode()).hexdigest(),
        "processed_at": datetime.now().isoformat(),
        "word_count": len(content.split()),
        "char_count": len(content)
    }
```

### 2. åˆ†å—ç­–ç•¥ä¼˜åŒ–

```python
def optimize_chunking_strategy(documents, kb_type="general"):
    """æ ¹æ®çŸ¥è¯†åº“ç±»å‹ä¼˜åŒ–åˆ†å—ç­–ç•¥"""

    chunking_configs = {
        "technical": TextChunkerConfig(
            chunk_size=1024,
            chunk_overlap=100,
            separator="\n## "  # æŒ‰æŠ€æœ¯æ–‡æ¡£æ ‡é¢˜åˆ†å‰²
        ),
        "legal": TextChunkerConfig(
            chunk_size=2048,
            chunk_overlap=200,
            separator="\n\n"  # æ³•å¾‹æ–‡æ¡£éœ€è¦æ›´å¤§çš„ä¸Šä¸‹æ–‡
        ),
        "faq": TextChunkerConfig(
            chunk_size=512,
            chunk_overlap=50,
            separator="\n\nQ:"  # FAQæŒ‰é—®é¢˜åˆ†å‰²
        ),
        "general": TextChunkerConfig(
            chunk_size=768,
            chunk_overlap=75,
            separator="\n\n"
        )
    }

    config = chunking_configs.get(kb_type, chunking_configs["general"])
    return TextChunker(config=config)

def adaptive_chunking(document_content, content_type):
    """è‡ªé€‚åº”åˆ†å—"""
    if "```" in document_content:  # åŒ…å«ä»£ç å—
        return TextChunker(config=TextChunkerConfig(
            chunk_size=1536,
            chunk_overlap=150,
            separator="\n```"
        ))
    elif document_content.count('\n#') > 10:  # Markdownæ–‡æ¡£
        return TextChunker(config=TextChunkerConfig(
            chunk_size=1024,
            chunk_overlap=100,
            separator="\n# "
        ))
    else:  # æ™®é€šæ–‡æœ¬
        return TextChunker(config=TextChunkerConfig(
            chunk_size=768,
            chunk_overlap=75
        ))
```

### 3. æŸ¥è¯¢ä¼˜åŒ–

```python
class QueryOptimizer:
    def __init__(self, knowledge_base):
        self.kb = knowledge_base
        self.query_cache = {}

    def optimize_query(self, query):
        """æŸ¥è¯¢ä¼˜åŒ–"""
        # 1. æŸ¥è¯¢ç¼“å­˜
        query_hash = hashlib.md5(query.encode()).hexdigest()
        if query_hash in self.query_cache:
            return self.query_cache[query_hash]

        # 2. æŸ¥è¯¢æ‰©å±•
        expanded_query = self.expand_query(query)

        # 3. å¤šç­–ç•¥æ£€ç´¢
        results = self.multi_strategy_retrieval(expanded_query)

        # 4. ç»“æœç¼“å­˜
        self.query_cache[query_hash] = results

        return results

    def expand_query(self, query):
        """æŸ¥è¯¢æ‰©å±•"""
        # æ·»åŠ åŒä¹‰è¯å’Œç›¸å…³è¯
        synonyms = {
            "AI": ["äººå·¥æ™ºèƒ½", "æœºå™¨æ™ºèƒ½"],
            "ML": ["æœºå™¨å­¦ä¹ ", "æœºå™¨å­¦ä¹ ç®—æ³•"],
            "DL": ["æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ"]
        }

        expanded_terms = [query]
        for term, syns in synonyms.items():
            if term.lower() in query.lower():
                expanded_terms.extend(syns)

        return " ".join(expanded_terms)

    def multi_strategy_retrieval(self, query):
        """å¤šç­–ç•¥æ£€ç´¢"""
        # 1. å‘é‡æ£€ç´¢
        vector_results = self.kb.search_documents(
            query=query,
            top_k=10,
            similarity_threshold=0.6
        )

        # 2. çŸ¥è¯†å›¾è°±æ£€ç´¢
        kg_results = self.kb.search_knowledge_graph(
            query=query,
            depth=2
        )

        # 3. ç»“æœèåˆ
        return self.fuse_results(vector_results, kg_results)

    def fuse_results(self, vector_results, kg_results):
        """ç»“æœèåˆ"""
        # ç®€å•çš„åˆ†æ•°åŠ æƒèåˆ
        fused_results = []

        for chunk in vector_results.chunks:
            score = chunk.score * 0.7  # å‘é‡æœç´¢æƒé‡

            # æ£€æŸ¥æ˜¯å¦åœ¨çŸ¥è¯†å›¾è°±ä¸­æœ‰ç›¸å…³å®ä½“
            for entity in kg_results.entities:
                if entity.name.lower() in chunk.text.lower():
                    score += 0.3  # çŸ¥è¯†å›¾è°±åŠ æƒ
                    break

            fused_results.append({
                "chunk": chunk,
                "fused_score": score
            })

        # æŒ‰èåˆåˆ†æ•°æ’åº
        fused_results.sort(key=lambda x: x["fused_score"], reverse=True)

        return fused_results[:5]  # è¿”å›top5
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹å¤„ç†ä¼˜åŒ–

```python
class BatchProcessor:
    def __init__(self, knowledge_base, batch_size=10):
        self.kb = knowledge_base
        self.batch_size = batch_size

    def batch_add_documents(self, file_paths):
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£"""
        total_files = len(file_paths)
        processed = 0

        for i in range(0, total_files, self.batch_size):
            batch = file_paths[i:i + self.batch_size]

            try:
                # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
                with ThreadPoolExecutor(max_workers=4) as executor:
                    futures = [
                        executor.submit(self.process_single_file, file_path)
                        for file_path in batch
                    ]

                    for future in as_completed(futures):
                        try:
                            result = future.result()
                            processed += 1
                            print(f"è¿›åº¦: {processed}/{total_files}")
                        except Exception as e:
                            print(f"å¤„ç†å¤±è´¥: {e}")

            except Exception as e:
                print(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")

    def process_single_file(self, file_path):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        return self.kb.add(file_path)

# ä½¿ç”¨ç¤ºä¾‹
batch_processor = BatchProcessor(kb, batch_size=5)
batch_processor.batch_add_documents(large_file_list)
```

### 2. ç¼“å­˜ç­–ç•¥

```python
import redis
import pickle
from functools import wraps

class CacheManager:
    def __init__(self, redis_url="redis://localhost:6379"):
        self.redis_client = redis.from_url(redis_url)
        self.default_ttl = 3600  # 1å°æ—¶

    def cache_result(self, ttl=None):
        """ç»“æœç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = self.generate_cache_key(func.__name__, args, kwargs)

                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = self.get_cached_result(cache_key)
                if cached_result is not None:
                    return cached_result

                # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
                result = func(*args, **kwargs)
                self.cache_result_data(cache_key, result, ttl or self.default_ttl)

                return result
            return wrapper
        return decorator

    def generate_cache_key(self, func_name, args, kwargs):
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        key_data = f"{func_name}:{str(args)}:{str(sorted(kwargs.items()))}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def get_cached_result(self, cache_key):
        """è·å–ç¼“å­˜ç»“æœ"""
        try:
            cached_data = self.redis_client.get(cache_key)
            if cached_data:
                return pickle.loads(cached_data)
        except Exception as e:
            print(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        return None

    def cache_result_data(self, cache_key, data, ttl):
        """ç¼“å­˜ç»“æœæ•°æ®"""
        try:
            serialized_data = pickle.dumps(data)
            self.redis_client.setex(cache_key, ttl, serialized_data)
        except Exception as e:
            print(f"ç¼“å­˜å†™å…¥å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
cache_manager = CacheManager()

class OptimizedKnowledgeBase:
    def __init__(self, kb):
        self.kb = kb

    @cache_manager.cache_result(ttl=1800)  # ç¼“å­˜30åˆ†é’Ÿ
    def cached_search(self, query, top_k=5):
        """å¸¦ç¼“å­˜çš„æœç´¢"""
        return self.kb.search_documents(query=query, top_k=top_k)

    @cache_manager.cache_result(ttl=3600)  # ç¼“å­˜1å°æ—¶
    def cached_kg_search(self, query, depth=2):
        """å¸¦ç¼“å­˜çš„çŸ¥è¯†å›¾è°±æœç´¢"""
        return self.kb.search_knowledge_graph(query=query, depth=depth)

# ä½¿ç”¨ä¼˜åŒ–åçš„çŸ¥è¯†åº“
optimized_kb = OptimizedKnowledgeBase(kb)
```

### 3. å¼‚æ­¥å¤„ç†

```python
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor

class AsyncAutoFlowClient:
    def __init__(self, base_url, api_key=None):
        self.base_url = base_url
        self.api_key = api_key
        self.session = None

    async def __aenter__(self):
        headers = {}
        if self.api_key:
            headers['Authorization'] = f'Bearer {self.api_key}'

        self.session = aiohttp.ClientSession(headers=headers)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def async_chat(self, messages, chat_engine="default"):
        """å¼‚æ­¥èŠå¤©"""
        url = f"{self.base_url}/api/chats"
        data = {
            "messages": messages,
            "chat_engine": chat_engine,
            "stream": False
        }

        async with self.session.post(url, json=data) as response:
            response.raise_for_status()
            return await response.json()

    async def batch_chat(self, message_batches):
        """æ‰¹é‡å¼‚æ­¥èŠå¤©"""
        tasks = [
            self.async_chat(messages)
            for messages in message_batches
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    async with AsyncAutoFlowClient("https://your-domain.com", "your-api-key") as client:
        # æ‰¹é‡å¤„ç†å¤šä¸ªæŸ¥è¯¢
        queries = [
            [{"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}],
            [{"role": "user", "content": "æ·±åº¦å­¦ä¹ çš„åº”ç”¨"}],
            [{"role": "user", "content": "AIçš„å‘å±•å†å²"}]
        ]

        results = await client.batch_chat(queries)

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"æŸ¥è¯¢ {i+1} å¤±è´¥: {result}")
            else:
                print(f"æŸ¥è¯¢ {i+1} ç»“æœ: {result['message']['content'][:100]}...")

# è¿è¡Œå¼‚æ­¥ç¤ºä¾‹
# asyncio.run(main())
```
```
