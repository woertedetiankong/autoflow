# AutoFlow æ•…éšœæ’é™¤æŒ‡å—

## ğŸ“‹ ç›®å½•

- [å®‰è£…é—®é¢˜](#å®‰è£…é—®é¢˜)
- [é…ç½®é—®é¢˜](#é…ç½®é—®é¢˜)
- [è¿è¡Œæ—¶é”™è¯¯](#è¿è¡Œæ—¶é”™è¯¯)
- [æ€§èƒ½é—®é¢˜](#æ€§èƒ½é—®é¢˜)
- [APIé”™è¯¯](#apié”™è¯¯)
- [æ•°æ®åº“é—®é¢˜](#æ•°æ®åº“é—®é¢˜)
- [å¸¸è§é—®é¢˜FAQ](#å¸¸è§é—®é¢˜faq)

## ğŸ”§ å®‰è£…é—®é¢˜

### 1. Docker Composeå¯åŠ¨å¤±è´¥

**é—®é¢˜**: `docker-compose up -d` å¤±è´¥

**å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆ**:

```bash
# æ£€æŸ¥Dockerç‰ˆæœ¬
docker --version
docker-compose --version

# ç¡®ä¿ç‰ˆæœ¬æ»¡è¶³è¦æ±‚
# Docker: 20.10+
# Docker Compose: 2.0+

# å¦‚æœç‰ˆæœ¬è¿‡ä½ï¼Œæ›´æ–°Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
```

**ç«¯å£å†²çªé—®é¢˜**:
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
sudo netstat -tulpn | grep :3000
sudo netstat -tulpn | grep :8000

# ä¿®æ”¹docker-compose.ymlä¸­çš„ç«¯å£æ˜ å°„
ports:
  - "3001:3000"  # å‰ç«¯ç«¯å£æ”¹ä¸º3001
  - "8001:80"    # åç«¯ç«¯å£æ”¹ä¸º8001
```

### 2. Python SDKå®‰è£…å¤±è´¥

**é—®é¢˜**: `pip install autoflow-ai` å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥Pythonç‰ˆæœ¬
python --version  # éœ€è¦3.10+

# å‡çº§pip
pip install --upgrade pip

# ä½¿ç”¨æ¸…åæºå®‰è£…
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple autoflow-ai

# å¦‚æœä»ç„¶å¤±è´¥ï¼Œä»æºç å®‰è£…
git clone https://github.com/pingcap/autoflow.git
cd autoflow/core
pip install -e .
```

### 3. ä¾èµ–å†²çª

**é—®é¢˜**: åŒ…ä¾èµ–å†²çª

**è§£å†³æ–¹æ¡ˆ**:
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv autoflow-env
source autoflow-env/bin/activate  # Linux/Mac
# æˆ–
autoflow-env\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install autoflow-ai

# æ£€æŸ¥ä¾èµ–å†²çª
pip check
```

## âš™ï¸ é…ç½®é—®é¢˜

### 1. æ•°æ®åº“è¿æ¥å¤±è´¥

**é”™è¯¯ä¿¡æ¯**: `Connection refused` æˆ– `Access denied`

**æ£€æŸ¥æ¸…å•**:
```bash
# 1. æ£€æŸ¥æ•°æ®åº“æœåŠ¡çŠ¶æ€
mysql -h your-host -P 4000 -u your-username -p

# 2. éªŒè¯ç¯å¢ƒå˜é‡
echo $TIDB_HOST
echo $TIDB_PORT
echo $TIDB_USERNAME

# 3. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
telnet your-tidb-host 4000

# 4. éªŒè¯SSLé…ç½®
# å¦‚æœä½¿ç”¨TiDB Cloudï¼Œç¡®ä¿å¯ç”¨SSL
TIDB_SSL_ENABLED=true
```

**TiDB Cloudè¿æ¥ç¤ºä¾‹**:
```bash
# .envæ–‡ä»¶é…ç½®
TIDB_HOST=gateway01.us-west-2.prod.aws.tidbcloud.com
TIDB_PORT=4000
TIDB_USERNAME=your-username
TIDB_PASSWORD=your-password
TIDB_DATABASE=test
TIDB_SSL_ENABLED=true
```

### 2. OpenAI APIé…ç½®é”™è¯¯

**é”™è¯¯ä¿¡æ¯**: `Invalid API key` æˆ– `Rate limit exceeded`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. éªŒè¯APIå¯†é’¥
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
     https://api.openai.com/v1/models

# 2. æ£€æŸ¥é…é¢å’Œé™åˆ¶
# ç™»å½•OpenAIæ§åˆ¶å°æŸ¥çœ‹ä½¿ç”¨æƒ…å†µ

# 3. ä½¿ç”¨ä»£ç†æˆ–å…¶ä»–æä¾›å•†
OPENAI_BASE_URL=https://your-proxy-url/v1
# æˆ–ä½¿ç”¨Azure OpenAI
OPENAI_API_TYPE=azure
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
```

### 3. ç¯å¢ƒå˜é‡æœªç”Ÿæ•ˆ

**é—®é¢˜**: é…ç½®çš„ç¯å¢ƒå˜é‡æ²¡æœ‰è¢«è¯»å–

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥.envæ–‡ä»¶ä½ç½®
ls -la .env

# 2. éªŒè¯æ–‡ä»¶æ ¼å¼ï¼ˆæ— BOMï¼ŒUnixæ¢è¡Œç¬¦ï¼‰
file .env
cat -A .env

# 3. é‡å¯æœåŠ¡
docker-compose down
docker-compose up -d

# 4. æ‰‹åŠ¨åŠ è½½ç¯å¢ƒå˜é‡
source .env
export $(cat .env | xargs)
```

## ğŸš¨ è¿è¡Œæ—¶é”™è¯¯

### 1. å†…å­˜ä¸è¶³é”™è¯¯

**é”™è¯¯ä¿¡æ¯**: `Out of memory` æˆ– `Killed`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥ç³»ç»Ÿå†…å­˜
free -h
top

# 2. è°ƒæ•´Dockerå†…å­˜é™åˆ¶
# åœ¨docker-compose.ymlä¸­æ·»åŠ 
services:
  backend:
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

# 3. ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
# åœ¨ä»£ç ä¸­å‡å°‘batch_size
batch_size = 5  # ä»10å‡å°‘åˆ°5
```

### 2. æ–‡æ¡£å¤„ç†å¤±è´¥

**é”™è¯¯ä¿¡æ¯**: `Failed to process document` æˆ– `Unsupported format`

**è¯Šæ–­æ­¥éª¤**:
```python
# 1. æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå¤§å°
import os
file_path = "problematic_file.pdf"
print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(file_path) / 1024 / 1024:.2f} MB")
print(f"æ–‡ä»¶æ‰©å±•å: {os.path.splitext(file_path)[1]}")

# 2. æµ‹è¯•æ–‡ä»¶è¯»å–
try:
    with open(file_path, 'rb') as f:
        content = f.read(1024)  # è¯»å–å‰1KB
    print("æ–‡ä»¶å¯è¯»")
except Exception as e:
    print(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")

# 3. ä½¿ç”¨æ›´å°çš„åˆ†å—å¤§å°
chunker = TextChunker(
    config=TextChunkerConfig(
        chunk_size=256,  # å‡å°åˆ†å—å¤§å°
        chunk_overlap=25
    )
)
```

### 3. çŸ¥è¯†å›¾è°±æå–å¤±è´¥

**é”™è¯¯ä¿¡æ¯**: `Knowledge graph extraction failed`

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. æ£€æŸ¥æ–‡æœ¬è´¨é‡
def check_text_quality(text):
    if len(text.strip()) < 50:
        return False, "æ–‡æœ¬è¿‡çŸ­"
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„å†…å®¹
    words = text.split()
    if len(words) < 10:
        return False, "è¯æ±‡é‡ä¸è¶³"
    
    return True, "æ–‡æœ¬è´¨é‡è‰¯å¥½"

# 2. é¢„å¤„ç†æ–‡æœ¬
def preprocess_for_kg(text):
    # ç§»é™¤è¿‡å¤šçš„æ¢è¡Œç¬¦
    text = re.sub(r'\n+', '\n', text)
    
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()[\]{}"\'-]', '', text)
    
    return text.strip()

# 3. åˆ†æ®µå¤„ç†
def process_in_segments(kb, long_text, segment_size=2000):
    segments = [long_text[i:i+segment_size] 
                for i in range(0, len(long_text), segment_size)]
    
    for i, segment in enumerate(segments):
        try:
            kb.add_text(segment)
            print(f"æ®µè½ {i+1} å¤„ç†æˆåŠŸ")
        except Exception as e:
            print(f"æ®µè½ {i+1} å¤„ç†å¤±è´¥: {e}")
```

## ğŸŒ æ€§èƒ½é—®é¢˜

### 1. æŸ¥è¯¢å“åº”æ…¢

**é—®é¢˜**: æœç´¢æˆ–é—®ç­”å“åº”æ—¶é—´è¿‡é•¿

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```python
# 1. å¯ç”¨æŸ¥è¯¢ç¼“å­˜
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_search(query, top_k=5):
    return kb.search_documents(query=query, top_k=top_k)

# 2. è°ƒæ•´æ£€ç´¢å‚æ•°
result = kb.search_documents(
    query=query,
    top_k=3,  # å‡å°‘è¿”å›æ•°é‡
    similarity_threshold=0.8  # æé«˜é˜ˆå€¼
)

# 3. ä½¿ç”¨å¼‚æ­¥å¤„ç†
import asyncio

async def async_search(queries):
    tasks = [search_single_query(q) for q in queries]
    return await asyncio.gather(*tasks)
```

### 2. å†…å­˜ä½¿ç”¨è¿‡é«˜

**ç›‘æ§å’Œä¼˜åŒ–**:
```python
import psutil
import gc

def monitor_memory():
    process = psutil.Process()
    memory_info = process.memory_info()
    print(f"å†…å­˜ä½¿ç”¨: {memory_info.rss / 1024 / 1024:.2f} MB")

# å®šæœŸæ¸…ç†å†…å­˜
def cleanup_memory():
    gc.collect()
    
# æ‰¹å¤„ç†ä¼˜åŒ–
def process_documents_in_batches(file_paths, batch_size=5):
    for i in range(0, len(file_paths), batch_size):
        batch = file_paths[i:i + batch_size]
        
        # å¤„ç†æ‰¹æ¬¡
        for file_path in batch:
            kb.add(file_path)
        
        # æ¸…ç†å†…å­˜
        cleanup_memory()
        monitor_memory()
```

### 3. æ•°æ®åº“æŸ¥è¯¢æ…¢

**ä¼˜åŒ–å»ºè®®**:
```sql
-- 1. æ£€æŸ¥ç´¢å¼•
SHOW INDEX FROM your_table;

-- 2. åˆ†ææŸ¥è¯¢è®¡åˆ’
EXPLAIN SELECT * FROM chunks WHERE embedding <-> '[...]' < 0.8;

-- 3. ä¼˜åŒ–å‘é‡ç´¢å¼•
-- ç¡®ä¿å‘é‡åˆ—æœ‰é€‚å½“çš„ç´¢å¼•
ALTER TABLE chunks ADD VECTOR INDEX idx_embedding (embedding);

-- 4. å®šæœŸæ›´æ–°ç»Ÿè®¡ä¿¡æ¯
ANALYZE TABLE chunks;
```

## ğŸŒ APIé”™è¯¯

### 1. è®¤è¯å¤±è´¥

**é”™è¯¯ç **: 401 Unauthorized

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. æ£€æŸ¥APIå¯†é’¥æ ¼å¼
api_key = "your-api-key"
if not api_key.startswith("sk-"):
    print("APIå¯†é’¥æ ¼å¼å¯èƒ½ä¸æ­£ç¡®")

# 2. éªŒè¯è¯·æ±‚å¤´
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}

# 3. æµ‹è¯•è®¤è¯
import requests
response = requests.get(
    "https://your-domain.com/api/healthz",
    headers=headers
)
print(f"è®¤è¯çŠ¶æ€: {response.status_code}")
```

### 2. è¯·æ±‚é¢‘ç‡é™åˆ¶

**é”™è¯¯ç **: 429 Too Many Requests

**è§£å†³æ–¹æ¡ˆ**:
```python
import time
import random
from functools import wraps

def rate_limit_retry(max_retries=3, base_delay=1):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except requests.exceptions.HTTPError as e:
                    if e.response.status_code == 429:
                        if attempt < max_retries - 1:
                            delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                            print(f"è¯·æ±‚é™åˆ¶ï¼Œç­‰å¾… {delay:.2f} ç§’åé‡è¯•...")
                            time.sleep(delay)
                            continue
                    raise
            return None
        return wrapper
    return decorator

@rate_limit_retry(max_retries=3)
def api_call():
    # ä½ çš„APIè°ƒç”¨ä»£ç 
    pass
```

### 3. æµå¼å“åº”ä¸­æ–­

**é—®é¢˜**: æµå¼èŠå¤©å“åº”æ„å¤–ä¸­æ–­

**è§£å†³æ–¹æ¡ˆ**:
```python
def robust_stream_chat(client, messages, max_retries=3):
    for attempt in range(max_retries):
        try:
            response_parts = []
            
            for event in client.chat(messages, stream=True):
                if event.get('event_type') == 'text_part':
                    response_parts.append(event['payload'])
                elif event.get('event_type') == 'error_part':
                    raise Exception(f"æµå¼å“åº”é”™è¯¯: {event['payload']}")
                elif event.get('event_type') == 'done':
                    return ''.join(response_parts)
            
        except Exception as e:
            print(f"å°è¯• {attempt + 1} å¤±è´¥: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                continue
            raise
    
    return None
```

## ğŸ—„ï¸ æ•°æ®åº“é—®é¢˜

### 1. è¿æ¥æ± è€—å°½

**é”™è¯¯ä¿¡æ¯**: `Connection pool exhausted`

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. è°ƒæ•´è¿æ¥æ± é…ç½®
from sqlalchemy import create_engine

engine = create_engine(
    database_url,
    pool_size=20,          # å¢åŠ è¿æ¥æ± å¤§å°
    max_overflow=30,       # å…è®¸çš„æº¢å‡ºè¿æ¥æ•°
    pool_timeout=30,       # è¿æ¥è¶…æ—¶æ—¶é—´
    pool_recycle=3600,     # è¿æ¥å›æ”¶æ—¶é—´
    pool_pre_ping=True     # è¿æ¥å‰pingæµ‹è¯•
)

# 2. ç¡®ä¿è¿æ¥æ­£ç¡®å…³é—­
from contextlib import contextmanager

@contextmanager
def get_db_session():
    session = Session(engine)
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()

# ä½¿ç”¨æ–¹å¼
with get_db_session() as session:
    # æ•°æ®åº“æ“ä½œ
    pass
```

### 2. å‘é‡ç´¢å¼•æ€§èƒ½é—®é¢˜

**é—®é¢˜**: å‘é‡æœç´¢å¾ˆæ…¢

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```sql
-- 1. æ£€æŸ¥å‘é‡ç´¢å¼•çŠ¶æ€
SHOW CREATE TABLE chunks;

-- 2. é‡å»ºå‘é‡ç´¢å¼•
ALTER TABLE chunks DROP INDEX idx_embedding;
ALTER TABLE chunks ADD VECTOR INDEX idx_embedding (embedding);

-- 3. è°ƒæ•´å‘é‡æœç´¢å‚æ•°
-- åœ¨åº”ç”¨ä¸­è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼
similarity_threshold = 0.8  # æé«˜é˜ˆå€¼å‡å°‘æœç´¢èŒƒå›´
top_k = 5  # å‡å°‘è¿”å›æ•°é‡
```

### 3. æ•°æ®ä¸€è‡´æ€§é—®é¢˜

**é—®é¢˜**: æ–‡æ¡£å’Œå‘é‡æ•°æ®ä¸ä¸€è‡´

**æ£€æŸ¥å’Œä¿®å¤**:
```python
def check_data_consistency(kb):
    """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
    # æ£€æŸ¥æ–‡æ¡£æ•°é‡
    doc_count = session.query(Document).count()
    
    # æ£€æŸ¥åˆ†å—æ•°é‡
    chunk_count = session.query(Chunk).count()
    
    # æ£€æŸ¥å‘é‡æ•°é‡
    vector_count = session.execute(
        "SELECT COUNT(*) FROM chunks WHERE embedding IS NOT NULL"
    ).scalar()
    
    print(f"æ–‡æ¡£æ•°é‡: {doc_count}")
    print(f"åˆ†å—æ•°é‡: {chunk_count}")
    print(f"å‘é‡æ•°é‡: {vector_count}")
    
    if chunk_count != vector_count:
        print("âš ï¸ è­¦å‘Š: åˆ†å—å’Œå‘é‡æ•°é‡ä¸ä¸€è‡´")
        return False
    
    return True

def repair_missing_vectors(kb):
    """ä¿®å¤ç¼ºå¤±çš„å‘é‡"""
    chunks_without_vectors = session.query(Chunk).filter(
        Chunk.embedding.is_(None)
    ).all()
    
    print(f"å‘ç° {len(chunks_without_vectors)} ä¸ªç¼ºå¤±å‘é‡çš„åˆ†å—")
    
    for chunk in chunks_without_vectors:
        try:
            # é‡æ–°ç”Ÿæˆå‘é‡
            embedding = kb._embedding_model.embed_text(chunk.text)
            chunk.embedding = embedding
            session.commit()
            print(f"ä¿®å¤åˆ†å— {chunk.id}")
        except Exception as e:
            print(f"ä¿®å¤åˆ†å— {chunk.id} å¤±è´¥: {e}")
```

## â“ å¸¸è§é—®é¢˜FAQ

### Q1: å¦‚ä½•é€‰æ‹©åˆé€‚çš„åˆ†å—å¤§å°ï¼Ÿ

**A**: åˆ†å—å¤§å°é€‰æ‹©æŒ‡å—ï¼š
- **æŠ€æœ¯æ–‡æ¡£**: 1024-1536 tokensï¼ˆä¿æŒä»£ç å®Œæ•´æ€§ï¼‰
- **æ³•å¾‹æ–‡æ¡£**: 2048+ tokensï¼ˆéœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
- **FAQæ–‡æ¡£**: 512 tokensï¼ˆé—®ç­”å¯¹ç›¸å¯¹ç‹¬ç«‹ï¼‰
- **ä¸€èˆ¬æ–‡æ¡£**: 768-1024 tokensï¼ˆå¹³è¡¡æ€§èƒ½å’Œè´¨é‡ï¼‰

### Q2: çŸ¥è¯†å›¾è°±å’Œå‘é‡æœç´¢å“ªä¸ªæ›´å¥½ï¼Ÿ

**A**: ä¸¤è€…å„æœ‰ä¼˜åŠ¿ï¼Œå»ºè®®ç»“åˆä½¿ç”¨ï¼š
- **å‘é‡æœç´¢**: è¯­ä¹‰ç›¸ä¼¼åº¦é«˜ï¼Œé€‚åˆæ¨¡ç³ŠæŸ¥è¯¢
- **çŸ¥è¯†å›¾è°±**: å…³ç³»æ˜ç¡®ï¼Œé€‚åˆç²¾ç¡®æŸ¥è¯¢
- **æ··åˆæ£€ç´¢**: ç»“åˆä¸¤è€…ä¼˜åŠ¿ï¼Œæä¾›æœ€ä½³ç»“æœ

### Q3: å¦‚ä½•æé«˜é—®ç­”è´¨é‡ï¼Ÿ

**A**: è´¨é‡æå‡ç­–ç•¥ï¼š
1. **æ–‡æ¡£è´¨é‡**: ç¡®ä¿æºæ–‡æ¡£å‡†ç¡®ã€å®Œæ•´
2. **åˆ†å—ç­–ç•¥**: ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
3. **æ¨¡å‹é€‰æ‹©**: ä½¿ç”¨æ›´å¼ºçš„LLMæ¨¡å‹
4. **æç¤ºä¼˜åŒ–**: ä¼˜åŒ–ç³»ç»Ÿæç¤ºè¯
5. **ç»“æœè¿‡æ»¤**: è®¾ç½®åˆé€‚çš„ç›¸ä¼¼åº¦é˜ˆå€¼

### Q4: æ”¯æŒå“ªäº›æ–‡æ¡£æ ¼å¼ï¼Ÿ

**A**: å½“å‰æ”¯æŒçš„æ ¼å¼ï¼š
- **æ–‡æœ¬**: .txt, .md, .csv
- **æ–‡æ¡£**: .pdf, .docx, .doc
- **ç½‘é¡µ**: .html, .htm
- **ä»£ç **: .py, .js, .java, .cpp
- **æ•°æ®**: .json, .xml, .yaml

### Q5: å¦‚ä½•ç›‘æ§ç³»ç»Ÿæ€§èƒ½ï¼Ÿ

**A**: ç›‘æ§å»ºè®®ï¼š
```python
# 1. å“åº”æ—¶é—´ç›‘æ§
import time

def monitor_response_time(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f"å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
        return result
    return wrapper

# 2. å†…å­˜ä½¿ç”¨ç›‘æ§
import psutil

def log_system_stats():
    cpu_percent = psutil.cpu_percent()
    memory = psutil.virtual_memory()
    print(f"CPU: {cpu_percent}%, å†…å­˜: {memory.percent}%")

# 3. æ•°æ®åº“è¿æ¥ç›‘æ§
def check_db_connections():
    result = session.execute("SHOW PROCESSLIST")
    connection_count = len(result.fetchall())
    print(f"æ•°æ®åº“è¿æ¥æ•°: {connection_count}")
```

---

## ğŸ†˜ è·å–å¸®åŠ©

å¦‚æœä»¥ä¸Šè§£å†³æ–¹æ¡ˆéƒ½æ— æ³•è§£å†³æ‚¨çš„é—®é¢˜ï¼Œè¯·ï¼š

1. **æŸ¥çœ‹æ—¥å¿—**: æ£€æŸ¥è¯¦ç»†çš„é”™è¯¯æ—¥å¿—
2. **æœç´¢Issues**: åœ¨GitHubä»“åº“ä¸­æœç´¢ç›¸ä¼¼é—®é¢˜
3. **æäº¤Issue**: æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œå¤ç°æ­¥éª¤
4. **ç¤¾åŒºè®¨è®º**: åœ¨Discordæˆ–è®¨è®ºåŒºå¯»æ±‚å¸®åŠ©

**æäº¤Issueæ—¶è¯·åŒ…å«**:
- é”™è¯¯ä¿¡æ¯å’Œå †æ ˆè·Ÿè¸ª
- ç³»ç»Ÿç¯å¢ƒä¿¡æ¯
- å¤ç°æ­¥éª¤
- ç›¸å…³é…ç½®æ–‡ä»¶ï¼ˆéšè—æ•æ„Ÿä¿¡æ¯ï¼‰
